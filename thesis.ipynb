{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from DISTS import DISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_model = DISTS().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "imsize = 256 if torch.cuda.is_available() else 64  # use small size if no GPU\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)), # scale imported image\n",
    "    transforms.ToTensor()                # transform it into a torch tensor\n",
    "])  \n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "def tensor_to_np(tensor):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def image_show(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "def image_save(tensor, folder, name):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    image.save(os.path.join(folder, name) + '.jpg', 'JPEG')\n",
    "\n",
    "\n",
    "def create_checkerboard(shape, square_size=8, device=None):\n",
    "    # Extract the height and width from the shape\n",
    "    if len(shape) == 4:  # Assuming shape is in the format (B, C, H, W)\n",
    "        _, _, rows, cols = shape\n",
    "    else:  # Assuming shape is in the format (C, H, W)\n",
    "        _, rows, cols = shape\n",
    "\n",
    "    # Create one row of the checkerboard\n",
    "    row_pattern = np.array([(i // square_size) % 2 for i in range(cols)])\n",
    "\n",
    "    # Create the full checkerboard\n",
    "    checkerboard = np.array([(row_pattern if i // square_size % 2 == 0 \n",
    "                              else 1 - row_pattern) for i in range(rows)])\n",
    "\n",
    "    # Convert to a PyTorch tensor and adjust device and data type\n",
    "    checkerboard_tensor = torch.from_numpy(checkerboard).float()\n",
    "    if device is not None:\n",
    "        checkerboard_tensor = checkerboard_tensor.to(device)\n",
    "\n",
    "    # Repeat across channel dimension and add a batch dimension\n",
    "    checkerboard_tensor = checkerboard_tensor.repeat(shape[1], 1, 1).unsqueeze(0)\n",
    "\n",
    "    return checkerboard_tensor\n",
    "\n",
    "\n",
    "def create_lines(shape, line_width=4, orientation='horizontal', device=None):\n",
    "    # Extract the height and width from the shape\n",
    "    if len(shape) == 4:  # Assuming shape is in the format (B, C, H, W)\n",
    "        _, _, rows, cols = shape\n",
    "    else:  # Assuming shape is in the format (C, H, W)\n",
    "        _, rows, cols = shape\n",
    "\n",
    "    # Create a single line pattern\n",
    "    single_line = np.zeros(line_width * 2)\n",
    "    single_line[:line_width] = 1\n",
    "\n",
    "    if orientation == 'horizontal':\n",
    "        # Tile the pattern vertically and then trim to match the image size\n",
    "        pattern = np.tile(single_line, cols // (line_width * 2) + 1)\n",
    "        pattern = pattern[:cols]\n",
    "        pattern = np.tile(pattern, (rows, 1))\n",
    "    else:  # vertical\n",
    "        # Tile the pattern horizontally and then trim to match the image size\n",
    "        pattern = np.tile(single_line, rows // (line_width * 2) + 1)\n",
    "        pattern = pattern[:rows]\n",
    "        pattern = np.tile(pattern, (cols, 1)).T\n",
    "\n",
    "    # Convert to a PyTorch tensor and adjust device and data type\n",
    "    lines_tensor = torch.from_numpy(pattern).float()\n",
    "    if device is not None:\n",
    "        lines_tensor = lines_tensor.to(device)\n",
    "\n",
    "    # Repeat across channel dimension and add a batch dimension\n",
    "    lines_tensor = lines_tensor.repeat(shape[1], 1, 1).unsqueeze(0)\n",
    "\n",
    "    return lines_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet feature map model\n",
    "class ResNetFeatureMapExtractor(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.selected_out = {}\n",
    "        self.pretrained = models.resnet34(weights='IMAGENET1K_V1').to(device).eval()\n",
    "        self.fhooks = []\n",
    "        \n",
    "        for name, module in list(self.pretrained.named_children()):\n",
    "            # forward hook for first ReLU layer\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                self.fhooks.append(module.register_forward_hook(self.forward_hook('relu')))\n",
    "            # attach forward hooks to every residual block\n",
    "            if isinstance(module, nn.Sequential):\n",
    "                for n, block in module.named_children():\n",
    "                    self.fhooks.append(block.register_forward_hook(self.forward_hook(f'{name}_{n}')))\n",
    "\n",
    "    def forward_hook(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.selected_out[layer_name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        # just return feature map activations\n",
    "        self.selected_out = {}\n",
    "        self.fhooks = []\n",
    "        self.pretrained(x)\n",
    "        return self.selected_out\n",
    "    \n",
    "\n",
    "# VGG feature map model\n",
    "class VGGFeatureMapExtractor(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.selected_out = {}\n",
    "        self.pretrained = models.vgg19(weights='IMAGENET1K_V1').features.to(device).eval()\n",
    "        self.fhooks = []\n",
    "\n",
    "        for name, module in list(self.pretrained.named_children()):\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                self.fhooks.append(module.register_forward_hook(self.forward_hook(f'relu_{name}')))\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                self.pretrained[int(name)] = nn.AvgPool2d(2)\n",
    "                self.fhooks.append(module.register_forward_hook(self.forward_hook(f'pool_{name}')))\n",
    "\n",
    "    def forward_hook(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.selected_out[layer_name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.selected_out = {}\n",
    "        self.fhooks = []\n",
    "        self.pretrained(x)\n",
    "        return self.selected_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(map):\n",
    "    b, f, w, h = map.size()\n",
    "    # flatten feature maps into matrix\n",
    "    features = map.view(b*f, w*h)\n",
    "    # calculate Gram matrix using transpose\n",
    "    return torch.mm(features, features.t())\n",
    "\n",
    "\n",
    "def synthesise(model, seed, exemplar, max_iter=200, stop_crit=1, _lr=1):\n",
    "    # optimise the seed and not the model parameters\n",
    "    seed.requires_grad_(True)\n",
    "    model.requires_grad_(False)\n",
    "\n",
    "    # init LBFGS\n",
    "    optimizer = optim.LBFGS([seed], lr=_lr)\n",
    "\n",
    "    # exemplar maps\n",
    "    exemplar_maps = model(exemplar)\n",
    "\n",
    "    loss_plot = []\n",
    "    grad_threshold = 0.001  # Threshold for the gradient below which the loop will break\n",
    "    window_size = 5  # Number of points to consider for calculating the gradient\n",
    "\n",
    "    # L-BFGS step\n",
    "    def closure():\n",
    "        # correct the values of updated input image\n",
    "        with torch.no_grad():\n",
    "            seed.clamp_(0, 1)\n",
    "\n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        seed_maps = model(seed)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        for (seed_map, exemplar_map) in zip(seed_maps.values(), exemplar_maps.values()):\n",
    "            b, f, w, h = seed_map.size()\n",
    "            seed_gram = gram(seed_map)\n",
    "            exemplar_gram = gram(exemplar_map)\n",
    "            loss += torch.sum(torch.square(torch.sub(exemplar_gram,\n",
    "                              seed_gram))) / (f**2*(w*h))\n",
    "        loss_plot.append(loss.item())\n",
    "        # print(loss.item())\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    for e in range(max_iter):\n",
    "        optimizer.step(closure)\n",
    "    \n",
    "        # Calculate the gradient of the loss\n",
    "        if len(loss_plot) > window_size:\n",
    "            recent_loss = loss_plot[-window_size:]\n",
    "            gradient = (recent_loss[0] - recent_loss[-1]) / window_size\n",
    "            \n",
    "            # Termination condition based on loss gradient\n",
    "            if abs(gradient) < stop_crit:\n",
    "                print('low loss gradient')\n",
    "                break\n",
    "\n",
    "        # Additional termination condition\n",
    "        if (loss_plot[-1] < 1e-5) or (math.isnan(loss_plot[-1])):\n",
    "            break\n",
    "\n",
    "    # final correction\n",
    "    with torch.no_grad():\n",
    "        seed.clamp_(0, 1)\n",
    "\n",
    "    return seed, loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run synthesis for a folder (`dataset`) with the following shape\n",
    "# dataset\n",
    "#   / images\n",
    "#       / texturecategory\n",
    "#           / texturename_001.jpg \n",
    "# saves results in `save_location`\n",
    "# seed_type is instance of the VGG or ResNet model\n",
    "# iterations is maximum iteration count\n",
    "# stop_crit is loss gradient threshold\n",
    "# kernel and sigmaval are used for optional blur for the seed\n",
    "# geo_w is used for the geometric seed component width\n",
    "def run_syn(cnn, dataset, save_location, seed_type, iterations, stop_crit=0.01, kernel=(3,3), sigmaval=1, geo_w=2):\n",
    "    # create folder for synthesised images\n",
    "    syn_img_path = os.path.join(save_location, 'images')\n",
    "    if not os.path.exists(syn_img_path):\n",
    "        os.makedirs(syn_img_path)\n",
    "\n",
    "    # create folder for DISTS scores\n",
    "    dists_score_path = os.path.join(save_location, 'scores')\n",
    "    if not os.path.exists(dists_score_path):\n",
    "        os.makedirs(dists_score_path)\n",
    "\n",
    "    # seed blur setting\n",
    "    blurrer = transforms.GaussianBlur(kernel_size=kernel, sigma=sigmaval)\n",
    "\n",
    "    # final DISTS scores\n",
    "    syn_scores = []\n",
    "\n",
    "    # get all exemplar category subfolders\n",
    "    dtd_subfolders = [f.path for f in os.scandir(dataset) if f.is_dir()]\n",
    "\n",
    "    # for every category\n",
    "    for texture_dir in dtd_subfolders:\n",
    "        \n",
    "        # get category title from path\n",
    "        name = os.path.basename(os.path.normpath(texture_dir))\n",
    "\n",
    "        # all images within a category\n",
    "        category_list = os.listdir(texture_dir)\n",
    "\n",
    "        for texture in category_list:\n",
    "            # retrieve actual image\n",
    "            full_path = f'{dataset}/{name}/{texture}'\n",
    "            # specific texture name\n",
    "            texture_name = os.path.splitext(texture)[0]\n",
    "\n",
    "            # load exemplar as tensor\n",
    "            exemplar = image_loader(full_path)\n",
    "            # default to Gaussian noise seed\n",
    "            seed = torch.randn(exemplar.shape).to(device)\n",
    "            if (seed_type == \"blur\"):\n",
    "                seed = blurrer(exemplar)\n",
    "            if (seed_type == \"lfnoise\"):\n",
    "                seed = blurrer(seed)\n",
    "            if (seed_type == \"square\"):\n",
    "                seed = create_checkerboard(exemplar.shape, geo_w)\n",
    "            if (seed_type == \"hline\"):\n",
    "                seed = create_lines(exemplar.shape, geo_w, 'horizontal')\n",
    "            if (seed_type == \"vline\"):\n",
    "                seed = create_lines(exemplar.shape, geo_w, 'vertical')\n",
    "\n",
    "            # save seed image\n",
    "            image_save(seed, os.path.join(save_location, 'images'), texture_name + \"_seed\")\n",
    "            # save exampler \n",
    "            image_save(exemplar, os.path.join(save_location, 'images'), texture_name + \"_example\")\n",
    "            # synthesise texture\n",
    "            output, loss_plot = synthesise(cnn, seed, exemplar, iterations, stop_crit)\n",
    "            # calculate DISTS score\n",
    "            similarity_score = dists_model(exemplar, output)\n",
    "            # Save synthesised image\n",
    "            image_save(output, os.path.join(save_location, 'images'), texture_name + \"_score=\" + str(similarity_score.item()))\n",
    "\n",
    "            print(texture_name)\n",
    "\n",
    "            # Construct loss plot with evenly spaced x-axis ticks\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(loss_plot)\n",
    "            ax.set(xlabel='iterations', ylabel='loss', title=name)\n",
    "            ax.grid()\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "            # Determine the number of ticks and their spacing\n",
    "            iterations = len(loss_plot)\n",
    "            max_ticks = 10\n",
    "            if iterations <= max_ticks:\n",
    "                step = 1\n",
    "            else:\n",
    "                step = np.ceil(iterations / max_ticks)\n",
    "                # Adjust step to ensure an even number of ticks\n",
    "                if (iterations // step) % 2 != 0:\n",
    "                    step += 1\n",
    "\n",
    "            # Set the x-ticks\n",
    "            x_ticks = np.arange(0, iterations, step)\n",
    "            ax.set_xticks(x_ticks)\n",
    "\n",
    "            fig.savefig(os.path.join(save_location, \"images\", texture_name + \"_lossplot.png\"))\n",
    "\n",
    "            # save DISTS score\n",
    "            syn_scores.append({'name': texture_name, 'score': similarity_score.item()})\n",
    "\n",
    "    # dicts to save the generated DISTS scores in different formats\n",
    "    ind_score = {}\n",
    "    cat_score = {}\n",
    "    sum_score = {}\n",
    "\n",
    "    # sort and save scores per category\n",
    "    for score in syn_scores:\n",
    "        ind_score[score['name']] = score['score']\n",
    "        cat = re.findall(\".*(?=_)\", score['name'])[0]\n",
    "        if cat in cat_score:\n",
    "            cat_score[cat].append(score['score'])\n",
    "        else:\n",
    "            cat_score[cat] = [score['score']]\n",
    "        if cat in sum_score:\n",
    "            sum_score[cat] += score['score']\n",
    "        else:\n",
    "            sum_score[cat] = score['score']\n",
    "\n",
    "    # calculate averages and sort\n",
    "    avg_score = {k: v / 4 for k, v in sum_score.items()}\n",
    "    sorted_score = {k: v for k, v in sorted(avg_score.items(), key=lambda item: item[1])}\n",
    "\n",
    "    # save score dicts as files\n",
    "    with open(os.path.join(dists_score_path, 'individual_scores'), 'wb') as f:\n",
    "        pickle.dump(ind_score, f)\n",
    "    with open(os.path.join(dists_score_path, 'categorical_scores'), 'wb') as f:\n",
    "        pickle.dump(cat_score, f)\n",
    "    with open(os.path.join(dists_score_path, 'averaged_scores'), 'wb') as f:\n",
    "        pickle.dump(sorted_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet = ResNetFeatureMapExtractor().to(device)\n",
    "VGG = VGGFeatureMapExtractor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_syn(VGG, './dtd_small/', './', 'noise', 100, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
